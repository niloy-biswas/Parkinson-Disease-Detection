{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>MDVP:Fo(Hz)</th>\n",
       "      <th>MDVP:Fhi(Hz)</th>\n",
       "      <th>MDVP:Flo(Hz)</th>\n",
       "      <th>MDVP:Jitter(%)</th>\n",
       "      <th>MDVP:Jitter(Abs)</th>\n",
       "      <th>MDVP:RAP</th>\n",
       "      <th>MDVP:PPQ</th>\n",
       "      <th>Jitter:DDP</th>\n",
       "      <th>MDVP:Shimmer</th>\n",
       "      <th>...</th>\n",
       "      <th>Shimmer:DDA</th>\n",
       "      <th>NHR</th>\n",
       "      <th>HNR</th>\n",
       "      <th>status</th>\n",
       "      <th>RPDE</th>\n",
       "      <th>DFA</th>\n",
       "      <th>spread1</th>\n",
       "      <th>spread2</th>\n",
       "      <th>D2</th>\n",
       "      <th>PPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>phon_R01_S01_1</td>\n",
       "      <td>119.992</td>\n",
       "      <td>157.302</td>\n",
       "      <td>74.997</td>\n",
       "      <td>0.00784</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>0.00370</td>\n",
       "      <td>0.00554</td>\n",
       "      <td>0.01109</td>\n",
       "      <td>0.04374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.06545</td>\n",
       "      <td>0.02211</td>\n",
       "      <td>21.033</td>\n",
       "      <td>1</td>\n",
       "      <td>0.414783</td>\n",
       "      <td>0.815285</td>\n",
       "      <td>-4.813031</td>\n",
       "      <td>0.266482</td>\n",
       "      <td>2.301442</td>\n",
       "      <td>0.284654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>phon_R01_S01_2</td>\n",
       "      <td>122.400</td>\n",
       "      <td>148.650</td>\n",
       "      <td>113.819</td>\n",
       "      <td>0.00968</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.00465</td>\n",
       "      <td>0.00696</td>\n",
       "      <td>0.01394</td>\n",
       "      <td>0.06134</td>\n",
       "      <td>...</td>\n",
       "      <td>0.09403</td>\n",
       "      <td>0.01929</td>\n",
       "      <td>19.085</td>\n",
       "      <td>1</td>\n",
       "      <td>0.458359</td>\n",
       "      <td>0.819521</td>\n",
       "      <td>-4.075192</td>\n",
       "      <td>0.335590</td>\n",
       "      <td>2.486855</td>\n",
       "      <td>0.368674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>phon_R01_S01_3</td>\n",
       "      <td>116.682</td>\n",
       "      <td>131.111</td>\n",
       "      <td>111.555</td>\n",
       "      <td>0.01050</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>0.00544</td>\n",
       "      <td>0.00781</td>\n",
       "      <td>0.01633</td>\n",
       "      <td>0.05233</td>\n",
       "      <td>...</td>\n",
       "      <td>0.08270</td>\n",
       "      <td>0.01309</td>\n",
       "      <td>20.651</td>\n",
       "      <td>1</td>\n",
       "      <td>0.429895</td>\n",
       "      <td>0.825288</td>\n",
       "      <td>-4.443179</td>\n",
       "      <td>0.311173</td>\n",
       "      <td>2.342259</td>\n",
       "      <td>0.332634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>phon_R01_S01_4</td>\n",
       "      <td>116.676</td>\n",
       "      <td>137.871</td>\n",
       "      <td>111.366</td>\n",
       "      <td>0.00997</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>0.00502</td>\n",
       "      <td>0.00698</td>\n",
       "      <td>0.01505</td>\n",
       "      <td>0.05492</td>\n",
       "      <td>...</td>\n",
       "      <td>0.08771</td>\n",
       "      <td>0.01353</td>\n",
       "      <td>20.644</td>\n",
       "      <td>1</td>\n",
       "      <td>0.434969</td>\n",
       "      <td>0.819235</td>\n",
       "      <td>-4.117501</td>\n",
       "      <td>0.334147</td>\n",
       "      <td>2.405554</td>\n",
       "      <td>0.368975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>phon_R01_S01_5</td>\n",
       "      <td>116.014</td>\n",
       "      <td>141.781</td>\n",
       "      <td>110.655</td>\n",
       "      <td>0.01284</td>\n",
       "      <td>0.00011</td>\n",
       "      <td>0.00655</td>\n",
       "      <td>0.00908</td>\n",
       "      <td>0.01966</td>\n",
       "      <td>0.06425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.10470</td>\n",
       "      <td>0.01767</td>\n",
       "      <td>19.649</td>\n",
       "      <td>1</td>\n",
       "      <td>0.417356</td>\n",
       "      <td>0.823484</td>\n",
       "      <td>-3.747787</td>\n",
       "      <td>0.234513</td>\n",
       "      <td>2.332180</td>\n",
       "      <td>0.410335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             name  MDVP:Fo(Hz)  MDVP:Fhi(Hz)  MDVP:Flo(Hz)  MDVP:Jitter(%)  \\\n",
       "0  phon_R01_S01_1      119.992       157.302        74.997         0.00784   \n",
       "1  phon_R01_S01_2      122.400       148.650       113.819         0.00968   \n",
       "2  phon_R01_S01_3      116.682       131.111       111.555         0.01050   \n",
       "3  phon_R01_S01_4      116.676       137.871       111.366         0.00997   \n",
       "4  phon_R01_S01_5      116.014       141.781       110.655         0.01284   \n",
       "\n",
       "   MDVP:Jitter(Abs)  MDVP:RAP  MDVP:PPQ  Jitter:DDP  MDVP:Shimmer  ...  \\\n",
       "0           0.00007   0.00370   0.00554     0.01109       0.04374  ...   \n",
       "1           0.00008   0.00465   0.00696     0.01394       0.06134  ...   \n",
       "2           0.00009   0.00544   0.00781     0.01633       0.05233  ...   \n",
       "3           0.00009   0.00502   0.00698     0.01505       0.05492  ...   \n",
       "4           0.00011   0.00655   0.00908     0.01966       0.06425  ...   \n",
       "\n",
       "   Shimmer:DDA      NHR     HNR  status      RPDE       DFA   spread1  \\\n",
       "0      0.06545  0.02211  21.033       1  0.414783  0.815285 -4.813031   \n",
       "1      0.09403  0.01929  19.085       1  0.458359  0.819521 -4.075192   \n",
       "2      0.08270  0.01309  20.651       1  0.429895  0.825288 -4.443179   \n",
       "3      0.08771  0.01353  20.644       1  0.434969  0.819235 -4.117501   \n",
       "4      0.10470  0.01767  19.649       1  0.417356  0.823484 -3.747787   \n",
       "\n",
       "    spread2        D2       PPE  \n",
       "0  0.266482  2.301442  0.284654  \n",
       "1  0.335590  2.486855  0.368674  \n",
       "2  0.311173  2.342259  0.332634  \n",
       "3  0.334147  2.405554  0.368975  \n",
       "4  0.234513  2.332180  0.410335  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"parkinsons.data\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(195, 24)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df1 = df.iloc[0:49, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>MDVP:Fo(Hz)</th>\n",
       "      <th>MDVP:Fhi(Hz)</th>\n",
       "      <th>MDVP:Flo(Hz)</th>\n",
       "      <th>MDVP:Jitter(%)</th>\n",
       "      <th>MDVP:Jitter(Abs)</th>\n",
       "      <th>MDVP:RAP</th>\n",
       "      <th>MDVP:PPQ</th>\n",
       "      <th>Jitter:DDP</th>\n",
       "      <th>MDVP:Shimmer</th>\n",
       "      <th>...</th>\n",
       "      <th>Shimmer:DDA</th>\n",
       "      <th>NHR</th>\n",
       "      <th>HNR</th>\n",
       "      <th>status</th>\n",
       "      <th>RPDE</th>\n",
       "      <th>DFA</th>\n",
       "      <th>spread1</th>\n",
       "      <th>spread2</th>\n",
       "      <th>D2</th>\n",
       "      <th>PPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>phon_R01_S01_1</td>\n",
       "      <td>119.992</td>\n",
       "      <td>157.302</td>\n",
       "      <td>74.997</td>\n",
       "      <td>0.00784</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.00370</td>\n",
       "      <td>0.00554</td>\n",
       "      <td>0.01109</td>\n",
       "      <td>0.04374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.06545</td>\n",
       "      <td>0.02211</td>\n",
       "      <td>21.033</td>\n",
       "      <td>1</td>\n",
       "      <td>0.414783</td>\n",
       "      <td>0.815285</td>\n",
       "      <td>-4.813031</td>\n",
       "      <td>0.266482</td>\n",
       "      <td>2.301442</td>\n",
       "      <td>0.284654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>phon_R01_S01_2</td>\n",
       "      <td>122.400</td>\n",
       "      <td>148.650</td>\n",
       "      <td>113.819</td>\n",
       "      <td>0.00968</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.00465</td>\n",
       "      <td>0.00696</td>\n",
       "      <td>0.01394</td>\n",
       "      <td>0.06134</td>\n",
       "      <td>...</td>\n",
       "      <td>0.09403</td>\n",
       "      <td>0.01929</td>\n",
       "      <td>19.085</td>\n",
       "      <td>1</td>\n",
       "      <td>0.458359</td>\n",
       "      <td>0.819521</td>\n",
       "      <td>-4.075192</td>\n",
       "      <td>0.335590</td>\n",
       "      <td>2.486855</td>\n",
       "      <td>0.368674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>phon_R01_S01_3</td>\n",
       "      <td>116.682</td>\n",
       "      <td>131.111</td>\n",
       "      <td>111.555</td>\n",
       "      <td>0.01050</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.00544</td>\n",
       "      <td>0.00781</td>\n",
       "      <td>0.01633</td>\n",
       "      <td>0.05233</td>\n",
       "      <td>...</td>\n",
       "      <td>0.08270</td>\n",
       "      <td>0.01309</td>\n",
       "      <td>20.651</td>\n",
       "      <td>1</td>\n",
       "      <td>0.429895</td>\n",
       "      <td>0.825288</td>\n",
       "      <td>-4.443179</td>\n",
       "      <td>0.311173</td>\n",
       "      <td>2.342259</td>\n",
       "      <td>0.332634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>phon_R01_S01_4</td>\n",
       "      <td>116.676</td>\n",
       "      <td>137.871</td>\n",
       "      <td>111.366</td>\n",
       "      <td>0.00997</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.00502</td>\n",
       "      <td>0.00698</td>\n",
       "      <td>0.01505</td>\n",
       "      <td>0.05492</td>\n",
       "      <td>...</td>\n",
       "      <td>0.08771</td>\n",
       "      <td>0.01353</td>\n",
       "      <td>20.644</td>\n",
       "      <td>1</td>\n",
       "      <td>0.434969</td>\n",
       "      <td>0.819235</td>\n",
       "      <td>-4.117501</td>\n",
       "      <td>0.334147</td>\n",
       "      <td>2.405554</td>\n",
       "      <td>0.368975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>phon_R01_S01_5</td>\n",
       "      <td>116.014</td>\n",
       "      <td>141.781</td>\n",
       "      <td>110.655</td>\n",
       "      <td>0.01284</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.00655</td>\n",
       "      <td>0.00908</td>\n",
       "      <td>0.01966</td>\n",
       "      <td>0.06425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.10470</td>\n",
       "      <td>0.01767</td>\n",
       "      <td>19.649</td>\n",
       "      <td>1</td>\n",
       "      <td>0.417356</td>\n",
       "      <td>0.823484</td>\n",
       "      <td>-3.747787</td>\n",
       "      <td>0.234513</td>\n",
       "      <td>2.332180</td>\n",
       "      <td>0.410335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>phon_R01_S01_6</td>\n",
       "      <td>120.552</td>\n",
       "      <td>131.162</td>\n",
       "      <td>113.787</td>\n",
       "      <td>0.00968</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.00463</td>\n",
       "      <td>0.00750</td>\n",
       "      <td>0.01388</td>\n",
       "      <td>0.04701</td>\n",
       "      <td>...</td>\n",
       "      <td>0.06985</td>\n",
       "      <td>0.01222</td>\n",
       "      <td>21.378</td>\n",
       "      <td>1</td>\n",
       "      <td>0.415564</td>\n",
       "      <td>0.825069</td>\n",
       "      <td>-4.242867</td>\n",
       "      <td>0.299111</td>\n",
       "      <td>2.187560</td>\n",
       "      <td>0.357775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>phon_R01_S02_1</td>\n",
       "      <td>120.267</td>\n",
       "      <td>137.244</td>\n",
       "      <td>114.820</td>\n",
       "      <td>0.00333</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.00155</td>\n",
       "      <td>0.00202</td>\n",
       "      <td>0.00466</td>\n",
       "      <td>0.01608</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02337</td>\n",
       "      <td>0.00607</td>\n",
       "      <td>24.886</td>\n",
       "      <td>1</td>\n",
       "      <td>0.596040</td>\n",
       "      <td>0.764112</td>\n",
       "      <td>-5.634322</td>\n",
       "      <td>0.257682</td>\n",
       "      <td>1.854785</td>\n",
       "      <td>0.211756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>phon_R01_S02_2</td>\n",
       "      <td>107.332</td>\n",
       "      <td>113.840</td>\n",
       "      <td>104.315</td>\n",
       "      <td>0.00290</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.00144</td>\n",
       "      <td>0.00182</td>\n",
       "      <td>0.00431</td>\n",
       "      <td>0.01567</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02487</td>\n",
       "      <td>0.00344</td>\n",
       "      <td>26.892</td>\n",
       "      <td>1</td>\n",
       "      <td>0.637420</td>\n",
       "      <td>0.763262</td>\n",
       "      <td>-6.167603</td>\n",
       "      <td>0.183721</td>\n",
       "      <td>2.064693</td>\n",
       "      <td>0.163755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>phon_R01_S02_3</td>\n",
       "      <td>95.730</td>\n",
       "      <td>132.068</td>\n",
       "      <td>91.754</td>\n",
       "      <td>0.00551</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.00293</td>\n",
       "      <td>0.00332</td>\n",
       "      <td>0.00880</td>\n",
       "      <td>0.02093</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03218</td>\n",
       "      <td>0.01070</td>\n",
       "      <td>21.812</td>\n",
       "      <td>1</td>\n",
       "      <td>0.615551</td>\n",
       "      <td>0.773587</td>\n",
       "      <td>-5.498678</td>\n",
       "      <td>0.327769</td>\n",
       "      <td>2.322511</td>\n",
       "      <td>0.231571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>phon_R01_S02_4</td>\n",
       "      <td>95.056</td>\n",
       "      <td>120.103</td>\n",
       "      <td>91.226</td>\n",
       "      <td>0.00532</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.00268</td>\n",
       "      <td>0.00332</td>\n",
       "      <td>0.00803</td>\n",
       "      <td>0.02838</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04324</td>\n",
       "      <td>0.01022</td>\n",
       "      <td>21.862</td>\n",
       "      <td>1</td>\n",
       "      <td>0.547037</td>\n",
       "      <td>0.798463</td>\n",
       "      <td>-5.011879</td>\n",
       "      <td>0.325996</td>\n",
       "      <td>2.432792</td>\n",
       "      <td>0.271362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>phon_R01_S02_5</td>\n",
       "      <td>88.333</td>\n",
       "      <td>112.240</td>\n",
       "      <td>84.072</td>\n",
       "      <td>0.00505</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.00254</td>\n",
       "      <td>0.00330</td>\n",
       "      <td>0.00763</td>\n",
       "      <td>0.02143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.01166</td>\n",
       "      <td>21.118</td>\n",
       "      <td>1</td>\n",
       "      <td>0.611137</td>\n",
       "      <td>0.776156</td>\n",
       "      <td>-5.249770</td>\n",
       "      <td>0.391002</td>\n",
       "      <td>2.407313</td>\n",
       "      <td>0.249740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>phon_R01_S02_6</td>\n",
       "      <td>91.904</td>\n",
       "      <td>115.871</td>\n",
       "      <td>86.292</td>\n",
       "      <td>0.00540</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.00281</td>\n",
       "      <td>0.00336</td>\n",
       "      <td>0.00844</td>\n",
       "      <td>0.02752</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04272</td>\n",
       "      <td>0.01141</td>\n",
       "      <td>21.414</td>\n",
       "      <td>1</td>\n",
       "      <td>0.583390</td>\n",
       "      <td>0.792520</td>\n",
       "      <td>-4.960234</td>\n",
       "      <td>0.363566</td>\n",
       "      <td>2.642476</td>\n",
       "      <td>0.275931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>phon_R01_S04_1</td>\n",
       "      <td>136.926</td>\n",
       "      <td>159.866</td>\n",
       "      <td>131.276</td>\n",
       "      <td>0.00293</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.00118</td>\n",
       "      <td>0.00153</td>\n",
       "      <td>0.00355</td>\n",
       "      <td>0.01259</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01968</td>\n",
       "      <td>0.00581</td>\n",
       "      <td>25.703</td>\n",
       "      <td>1</td>\n",
       "      <td>0.460600</td>\n",
       "      <td>0.646846</td>\n",
       "      <td>-6.547148</td>\n",
       "      <td>0.152813</td>\n",
       "      <td>2.041277</td>\n",
       "      <td>0.138512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>phon_R01_S04_2</td>\n",
       "      <td>139.173</td>\n",
       "      <td>179.139</td>\n",
       "      <td>76.556</td>\n",
       "      <td>0.00390</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.00165</td>\n",
       "      <td>0.00208</td>\n",
       "      <td>0.00496</td>\n",
       "      <td>0.01642</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02184</td>\n",
       "      <td>0.01041</td>\n",
       "      <td>24.889</td>\n",
       "      <td>1</td>\n",
       "      <td>0.430166</td>\n",
       "      <td>0.665833</td>\n",
       "      <td>-5.660217</td>\n",
       "      <td>0.254989</td>\n",
       "      <td>2.519422</td>\n",
       "      <td>0.199889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>phon_R01_S04_3</td>\n",
       "      <td>152.845</td>\n",
       "      <td>163.305</td>\n",
       "      <td>75.836</td>\n",
       "      <td>0.00294</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.00121</td>\n",
       "      <td>0.00149</td>\n",
       "      <td>0.00364</td>\n",
       "      <td>0.01828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03191</td>\n",
       "      <td>0.00609</td>\n",
       "      <td>24.922</td>\n",
       "      <td>1</td>\n",
       "      <td>0.474791</td>\n",
       "      <td>0.654027</td>\n",
       "      <td>-6.105098</td>\n",
       "      <td>0.203653</td>\n",
       "      <td>2.125618</td>\n",
       "      <td>0.170100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>phon_R01_S04_4</td>\n",
       "      <td>142.167</td>\n",
       "      <td>217.455</td>\n",
       "      <td>83.159</td>\n",
       "      <td>0.00369</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.00157</td>\n",
       "      <td>0.00203</td>\n",
       "      <td>0.00471</td>\n",
       "      <td>0.01503</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02316</td>\n",
       "      <td>0.00839</td>\n",
       "      <td>25.175</td>\n",
       "      <td>1</td>\n",
       "      <td>0.565924</td>\n",
       "      <td>0.658245</td>\n",
       "      <td>-5.340115</td>\n",
       "      <td>0.210185</td>\n",
       "      <td>2.205546</td>\n",
       "      <td>0.234589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>phon_R01_S04_5</td>\n",
       "      <td>144.188</td>\n",
       "      <td>349.259</td>\n",
       "      <td>82.764</td>\n",
       "      <td>0.00544</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.00211</td>\n",
       "      <td>0.00292</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>0.02047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02908</td>\n",
       "      <td>0.01859</td>\n",
       "      <td>22.333</td>\n",
       "      <td>1</td>\n",
       "      <td>0.567380</td>\n",
       "      <td>0.644692</td>\n",
       "      <td>-5.440040</td>\n",
       "      <td>0.239764</td>\n",
       "      <td>2.264501</td>\n",
       "      <td>0.218164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>phon_R01_S04_6</td>\n",
       "      <td>168.778</td>\n",
       "      <td>232.181</td>\n",
       "      <td>75.603</td>\n",
       "      <td>0.00718</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.00284</td>\n",
       "      <td>0.00387</td>\n",
       "      <td>0.00853</td>\n",
       "      <td>0.03327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04322</td>\n",
       "      <td>0.02919</td>\n",
       "      <td>20.376</td>\n",
       "      <td>1</td>\n",
       "      <td>0.631099</td>\n",
       "      <td>0.605417</td>\n",
       "      <td>-2.931070</td>\n",
       "      <td>0.434326</td>\n",
       "      <td>3.007463</td>\n",
       "      <td>0.430788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>phon_R01_S05_1</td>\n",
       "      <td>153.046</td>\n",
       "      <td>175.829</td>\n",
       "      <td>68.623</td>\n",
       "      <td>0.00742</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.00364</td>\n",
       "      <td>0.00432</td>\n",
       "      <td>0.01092</td>\n",
       "      <td>0.05517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.07413</td>\n",
       "      <td>0.03160</td>\n",
       "      <td>17.280</td>\n",
       "      <td>1</td>\n",
       "      <td>0.665318</td>\n",
       "      <td>0.719467</td>\n",
       "      <td>-3.949079</td>\n",
       "      <td>0.357870</td>\n",
       "      <td>3.109010</td>\n",
       "      <td>0.377429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>phon_R01_S05_2</td>\n",
       "      <td>156.405</td>\n",
       "      <td>189.398</td>\n",
       "      <td>142.822</td>\n",
       "      <td>0.00768</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.00372</td>\n",
       "      <td>0.00399</td>\n",
       "      <td>0.01116</td>\n",
       "      <td>0.03995</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05164</td>\n",
       "      <td>0.03365</td>\n",
       "      <td>17.153</td>\n",
       "      <td>1</td>\n",
       "      <td>0.649554</td>\n",
       "      <td>0.686080</td>\n",
       "      <td>-4.554466</td>\n",
       "      <td>0.340176</td>\n",
       "      <td>2.856676</td>\n",
       "      <td>0.322111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>phon_R01_S05_3</td>\n",
       "      <td>153.848</td>\n",
       "      <td>165.738</td>\n",
       "      <td>65.782</td>\n",
       "      <td>0.00840</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.00428</td>\n",
       "      <td>0.00450</td>\n",
       "      <td>0.01285</td>\n",
       "      <td>0.03810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.03871</td>\n",
       "      <td>17.536</td>\n",
       "      <td>1</td>\n",
       "      <td>0.660125</td>\n",
       "      <td>0.704087</td>\n",
       "      <td>-4.095442</td>\n",
       "      <td>0.262564</td>\n",
       "      <td>2.739710</td>\n",
       "      <td>0.365391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>phon_R01_S05_4</td>\n",
       "      <td>153.880</td>\n",
       "      <td>172.860</td>\n",
       "      <td>78.128</td>\n",
       "      <td>0.00480</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.00232</td>\n",
       "      <td>0.00267</td>\n",
       "      <td>0.00696</td>\n",
       "      <td>0.04137</td>\n",
       "      <td>...</td>\n",
       "      <td>0.06062</td>\n",
       "      <td>0.01849</td>\n",
       "      <td>19.493</td>\n",
       "      <td>1</td>\n",
       "      <td>0.629017</td>\n",
       "      <td>0.698951</td>\n",
       "      <td>-5.186960</td>\n",
       "      <td>0.237622</td>\n",
       "      <td>2.557536</td>\n",
       "      <td>0.259765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>phon_R01_S05_5</td>\n",
       "      <td>167.930</td>\n",
       "      <td>193.221</td>\n",
       "      <td>79.068</td>\n",
       "      <td>0.00442</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.00220</td>\n",
       "      <td>0.00247</td>\n",
       "      <td>0.00661</td>\n",
       "      <td>0.04351</td>\n",
       "      <td>...</td>\n",
       "      <td>0.06685</td>\n",
       "      <td>0.01280</td>\n",
       "      <td>22.468</td>\n",
       "      <td>1</td>\n",
       "      <td>0.619060</td>\n",
       "      <td>0.679834</td>\n",
       "      <td>-4.330956</td>\n",
       "      <td>0.262384</td>\n",
       "      <td>2.916777</td>\n",
       "      <td>0.285695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>phon_R01_S05_6</td>\n",
       "      <td>173.917</td>\n",
       "      <td>192.735</td>\n",
       "      <td>86.180</td>\n",
       "      <td>0.00476</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.00221</td>\n",
       "      <td>0.00258</td>\n",
       "      <td>0.00663</td>\n",
       "      <td>0.04192</td>\n",
       "      <td>...</td>\n",
       "      <td>0.06562</td>\n",
       "      <td>0.01840</td>\n",
       "      <td>20.422</td>\n",
       "      <td>1</td>\n",
       "      <td>0.537264</td>\n",
       "      <td>0.686894</td>\n",
       "      <td>-5.248776</td>\n",
       "      <td>0.210279</td>\n",
       "      <td>2.547508</td>\n",
       "      <td>0.253556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>phon_R01_S06_1</td>\n",
       "      <td>163.656</td>\n",
       "      <td>200.841</td>\n",
       "      <td>76.779</td>\n",
       "      <td>0.00742</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.00380</td>\n",
       "      <td>0.00390</td>\n",
       "      <td>0.01140</td>\n",
       "      <td>0.01659</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02214</td>\n",
       "      <td>0.01778</td>\n",
       "      <td>23.831</td>\n",
       "      <td>1</td>\n",
       "      <td>0.397937</td>\n",
       "      <td>0.732479</td>\n",
       "      <td>-5.557447</td>\n",
       "      <td>0.220890</td>\n",
       "      <td>2.692176</td>\n",
       "      <td>0.215961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>phon_R01_S06_2</td>\n",
       "      <td>104.400</td>\n",
       "      <td>206.002</td>\n",
       "      <td>77.968</td>\n",
       "      <td>0.00633</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.00316</td>\n",
       "      <td>0.00375</td>\n",
       "      <td>0.00948</td>\n",
       "      <td>0.03767</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05197</td>\n",
       "      <td>0.02887</td>\n",
       "      <td>22.066</td>\n",
       "      <td>1</td>\n",
       "      <td>0.522746</td>\n",
       "      <td>0.737948</td>\n",
       "      <td>-5.571843</td>\n",
       "      <td>0.236853</td>\n",
       "      <td>2.846369</td>\n",
       "      <td>0.219514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>phon_R01_S06_3</td>\n",
       "      <td>171.041</td>\n",
       "      <td>208.313</td>\n",
       "      <td>75.501</td>\n",
       "      <td>0.00455</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.00250</td>\n",
       "      <td>0.00234</td>\n",
       "      <td>0.00750</td>\n",
       "      <td>0.01966</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02666</td>\n",
       "      <td>0.01095</td>\n",
       "      <td>25.908</td>\n",
       "      <td>1</td>\n",
       "      <td>0.418622</td>\n",
       "      <td>0.720916</td>\n",
       "      <td>-6.183590</td>\n",
       "      <td>0.226278</td>\n",
       "      <td>2.589702</td>\n",
       "      <td>0.147403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>phon_R01_S06_4</td>\n",
       "      <td>146.845</td>\n",
       "      <td>208.701</td>\n",
       "      <td>81.737</td>\n",
       "      <td>0.00496</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.00250</td>\n",
       "      <td>0.00275</td>\n",
       "      <td>0.00749</td>\n",
       "      <td>0.01919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02650</td>\n",
       "      <td>0.01328</td>\n",
       "      <td>25.119</td>\n",
       "      <td>1</td>\n",
       "      <td>0.358773</td>\n",
       "      <td>0.726652</td>\n",
       "      <td>-6.271690</td>\n",
       "      <td>0.196102</td>\n",
       "      <td>2.314209</td>\n",
       "      <td>0.162999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>phon_R01_S06_5</td>\n",
       "      <td>155.358</td>\n",
       "      <td>227.383</td>\n",
       "      <td>80.055</td>\n",
       "      <td>0.00310</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.00159</td>\n",
       "      <td>0.00176</td>\n",
       "      <td>0.00476</td>\n",
       "      <td>0.01718</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02307</td>\n",
       "      <td>0.00677</td>\n",
       "      <td>25.970</td>\n",
       "      <td>1</td>\n",
       "      <td>0.470478</td>\n",
       "      <td>0.676258</td>\n",
       "      <td>-7.120925</td>\n",
       "      <td>0.279789</td>\n",
       "      <td>2.241742</td>\n",
       "      <td>0.108514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>phon_R01_S06_6</td>\n",
       "      <td>162.568</td>\n",
       "      <td>198.346</td>\n",
       "      <td>77.630</td>\n",
       "      <td>0.00502</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.00280</td>\n",
       "      <td>0.00253</td>\n",
       "      <td>0.00841</td>\n",
       "      <td>0.01791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02380</td>\n",
       "      <td>0.01170</td>\n",
       "      <td>25.678</td>\n",
       "      <td>1</td>\n",
       "      <td>0.427785</td>\n",
       "      <td>0.723797</td>\n",
       "      <td>-6.635729</td>\n",
       "      <td>0.209866</td>\n",
       "      <td>1.957961</td>\n",
       "      <td>0.135242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>phon_R01_S07_1</td>\n",
       "      <td>197.076</td>\n",
       "      <td>206.896</td>\n",
       "      <td>192.055</td>\n",
       "      <td>0.00289</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.00166</td>\n",
       "      <td>0.00168</td>\n",
       "      <td>0.00498</td>\n",
       "      <td>0.01098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01689</td>\n",
       "      <td>0.00339</td>\n",
       "      <td>26.775</td>\n",
       "      <td>0</td>\n",
       "      <td>0.422229</td>\n",
       "      <td>0.741367</td>\n",
       "      <td>-7.348300</td>\n",
       "      <td>0.177551</td>\n",
       "      <td>1.743867</td>\n",
       "      <td>0.085569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>phon_R01_S07_2</td>\n",
       "      <td>199.228</td>\n",
       "      <td>209.512</td>\n",
       "      <td>192.091</td>\n",
       "      <td>0.00241</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.00134</td>\n",
       "      <td>0.00138</td>\n",
       "      <td>0.00402</td>\n",
       "      <td>0.01015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01513</td>\n",
       "      <td>0.00167</td>\n",
       "      <td>30.940</td>\n",
       "      <td>0</td>\n",
       "      <td>0.432439</td>\n",
       "      <td>0.742055</td>\n",
       "      <td>-7.682587</td>\n",
       "      <td>0.173319</td>\n",
       "      <td>2.103106</td>\n",
       "      <td>0.068501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>phon_R01_S07_3</td>\n",
       "      <td>198.383</td>\n",
       "      <td>215.203</td>\n",
       "      <td>193.104</td>\n",
       "      <td>0.00212</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.00113</td>\n",
       "      <td>0.00135</td>\n",
       "      <td>0.00339</td>\n",
       "      <td>0.01263</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01919</td>\n",
       "      <td>0.00119</td>\n",
       "      <td>30.775</td>\n",
       "      <td>0</td>\n",
       "      <td>0.465946</td>\n",
       "      <td>0.738703</td>\n",
       "      <td>-7.067931</td>\n",
       "      <td>0.175181</td>\n",
       "      <td>1.512275</td>\n",
       "      <td>0.096320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>phon_R01_S07_4</td>\n",
       "      <td>202.266</td>\n",
       "      <td>211.604</td>\n",
       "      <td>197.079</td>\n",
       "      <td>0.00180</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.00093</td>\n",
       "      <td>0.00107</td>\n",
       "      <td>0.00278</td>\n",
       "      <td>0.00954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01407</td>\n",
       "      <td>0.00072</td>\n",
       "      <td>32.684</td>\n",
       "      <td>0</td>\n",
       "      <td>0.368535</td>\n",
       "      <td>0.742133</td>\n",
       "      <td>-7.695734</td>\n",
       "      <td>0.178540</td>\n",
       "      <td>1.544609</td>\n",
       "      <td>0.056141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>phon_R01_S07_5</td>\n",
       "      <td>203.184</td>\n",
       "      <td>211.526</td>\n",
       "      <td>196.160</td>\n",
       "      <td>0.00178</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.00094</td>\n",
       "      <td>0.00106</td>\n",
       "      <td>0.00283</td>\n",
       "      <td>0.00958</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01403</td>\n",
       "      <td>0.00065</td>\n",
       "      <td>33.047</td>\n",
       "      <td>0</td>\n",
       "      <td>0.340068</td>\n",
       "      <td>0.741899</td>\n",
       "      <td>-7.964984</td>\n",
       "      <td>0.163519</td>\n",
       "      <td>1.423287</td>\n",
       "      <td>0.044539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>phon_R01_S07_6</td>\n",
       "      <td>201.464</td>\n",
       "      <td>210.565</td>\n",
       "      <td>195.708</td>\n",
       "      <td>0.00198</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.00105</td>\n",
       "      <td>0.00115</td>\n",
       "      <td>0.00314</td>\n",
       "      <td>0.01194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01758</td>\n",
       "      <td>0.00135</td>\n",
       "      <td>31.732</td>\n",
       "      <td>0</td>\n",
       "      <td>0.344252</td>\n",
       "      <td>0.742737</td>\n",
       "      <td>-7.777685</td>\n",
       "      <td>0.170183</td>\n",
       "      <td>2.447064</td>\n",
       "      <td>0.057610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>phon_R01_S08_1</td>\n",
       "      <td>177.876</td>\n",
       "      <td>192.921</td>\n",
       "      <td>168.013</td>\n",
       "      <td>0.00411</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.00233</td>\n",
       "      <td>0.00241</td>\n",
       "      <td>0.00700</td>\n",
       "      <td>0.02126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03463</td>\n",
       "      <td>0.00586</td>\n",
       "      <td>23.216</td>\n",
       "      <td>1</td>\n",
       "      <td>0.360148</td>\n",
       "      <td>0.778834</td>\n",
       "      <td>-6.149653</td>\n",
       "      <td>0.218037</td>\n",
       "      <td>2.477082</td>\n",
       "      <td>0.165827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>phon_R01_S08_2</td>\n",
       "      <td>176.170</td>\n",
       "      <td>185.604</td>\n",
       "      <td>163.564</td>\n",
       "      <td>0.00369</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.00205</td>\n",
       "      <td>0.00218</td>\n",
       "      <td>0.00616</td>\n",
       "      <td>0.01851</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02814</td>\n",
       "      <td>0.00340</td>\n",
       "      <td>24.951</td>\n",
       "      <td>1</td>\n",
       "      <td>0.341435</td>\n",
       "      <td>0.783626</td>\n",
       "      <td>-6.006414</td>\n",
       "      <td>0.196371</td>\n",
       "      <td>2.536527</td>\n",
       "      <td>0.173218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>phon_R01_S08_3</td>\n",
       "      <td>180.198</td>\n",
       "      <td>201.249</td>\n",
       "      <td>175.456</td>\n",
       "      <td>0.00284</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.00153</td>\n",
       "      <td>0.00166</td>\n",
       "      <td>0.00459</td>\n",
       "      <td>0.01444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02177</td>\n",
       "      <td>0.00231</td>\n",
       "      <td>26.738</td>\n",
       "      <td>1</td>\n",
       "      <td>0.403884</td>\n",
       "      <td>0.766209</td>\n",
       "      <td>-6.452058</td>\n",
       "      <td>0.212294</td>\n",
       "      <td>2.269398</td>\n",
       "      <td>0.141929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>phon_R01_S08_4</td>\n",
       "      <td>187.733</td>\n",
       "      <td>202.324</td>\n",
       "      <td>173.015</td>\n",
       "      <td>0.00316</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.00168</td>\n",
       "      <td>0.00182</td>\n",
       "      <td>0.00504</td>\n",
       "      <td>0.01663</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02488</td>\n",
       "      <td>0.00265</td>\n",
       "      <td>26.310</td>\n",
       "      <td>1</td>\n",
       "      <td>0.396793</td>\n",
       "      <td>0.758324</td>\n",
       "      <td>-6.006647</td>\n",
       "      <td>0.266892</td>\n",
       "      <td>2.382544</td>\n",
       "      <td>0.160691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>phon_R01_S08_5</td>\n",
       "      <td>186.163</td>\n",
       "      <td>197.724</td>\n",
       "      <td>177.584</td>\n",
       "      <td>0.00298</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.00165</td>\n",
       "      <td>0.00175</td>\n",
       "      <td>0.00496</td>\n",
       "      <td>0.01495</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02321</td>\n",
       "      <td>0.00231</td>\n",
       "      <td>26.822</td>\n",
       "      <td>1</td>\n",
       "      <td>0.326480</td>\n",
       "      <td>0.765623</td>\n",
       "      <td>-6.647379</td>\n",
       "      <td>0.201095</td>\n",
       "      <td>2.374073</td>\n",
       "      <td>0.130554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>phon_R01_S08_6</td>\n",
       "      <td>184.055</td>\n",
       "      <td>196.537</td>\n",
       "      <td>166.977</td>\n",
       "      <td>0.00258</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.00134</td>\n",
       "      <td>0.00147</td>\n",
       "      <td>0.00403</td>\n",
       "      <td>0.01463</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02226</td>\n",
       "      <td>0.00257</td>\n",
       "      <td>26.453</td>\n",
       "      <td>1</td>\n",
       "      <td>0.306443</td>\n",
       "      <td>0.759203</td>\n",
       "      <td>-7.044105</td>\n",
       "      <td>0.063412</td>\n",
       "      <td>2.361532</td>\n",
       "      <td>0.115730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>phon_R01_S10_1</td>\n",
       "      <td>237.226</td>\n",
       "      <td>247.326</td>\n",
       "      <td>225.227</td>\n",
       "      <td>0.00298</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.00169</td>\n",
       "      <td>0.00182</td>\n",
       "      <td>0.00507</td>\n",
       "      <td>0.01752</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03104</td>\n",
       "      <td>0.00740</td>\n",
       "      <td>22.736</td>\n",
       "      <td>0</td>\n",
       "      <td>0.305062</td>\n",
       "      <td>0.654172</td>\n",
       "      <td>-7.310550</td>\n",
       "      <td>0.098648</td>\n",
       "      <td>2.416838</td>\n",
       "      <td>0.095032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>phon_R01_S10_2</td>\n",
       "      <td>241.404</td>\n",
       "      <td>248.834</td>\n",
       "      <td>232.483</td>\n",
       "      <td>0.00281</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.00157</td>\n",
       "      <td>0.00173</td>\n",
       "      <td>0.00470</td>\n",
       "      <td>0.01760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03017</td>\n",
       "      <td>0.00675</td>\n",
       "      <td>23.145</td>\n",
       "      <td>0</td>\n",
       "      <td>0.457702</td>\n",
       "      <td>0.634267</td>\n",
       "      <td>-6.793547</td>\n",
       "      <td>0.158266</td>\n",
       "      <td>2.256699</td>\n",
       "      <td>0.117399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>phon_R01_S10_3</td>\n",
       "      <td>243.439</td>\n",
       "      <td>250.912</td>\n",
       "      <td>232.435</td>\n",
       "      <td>0.00210</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.00109</td>\n",
       "      <td>0.00137</td>\n",
       "      <td>0.00327</td>\n",
       "      <td>0.01419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02330</td>\n",
       "      <td>0.00454</td>\n",
       "      <td>25.368</td>\n",
       "      <td>0</td>\n",
       "      <td>0.438296</td>\n",
       "      <td>0.635285</td>\n",
       "      <td>-7.057869</td>\n",
       "      <td>0.091608</td>\n",
       "      <td>2.330716</td>\n",
       "      <td>0.091470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>phon_R01_S10_4</td>\n",
       "      <td>242.852</td>\n",
       "      <td>255.034</td>\n",
       "      <td>227.911</td>\n",
       "      <td>0.00225</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.00117</td>\n",
       "      <td>0.00139</td>\n",
       "      <td>0.00350</td>\n",
       "      <td>0.01494</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02542</td>\n",
       "      <td>0.00476</td>\n",
       "      <td>25.032</td>\n",
       "      <td>0</td>\n",
       "      <td>0.431285</td>\n",
       "      <td>0.638928</td>\n",
       "      <td>-6.995820</td>\n",
       "      <td>0.102083</td>\n",
       "      <td>2.365800</td>\n",
       "      <td>0.102706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>phon_R01_S10_5</td>\n",
       "      <td>245.510</td>\n",
       "      <td>262.090</td>\n",
       "      <td>231.848</td>\n",
       "      <td>0.00235</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.00127</td>\n",
       "      <td>0.00148</td>\n",
       "      <td>0.00380</td>\n",
       "      <td>0.01608</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02719</td>\n",
       "      <td>0.00476</td>\n",
       "      <td>24.602</td>\n",
       "      <td>0</td>\n",
       "      <td>0.467489</td>\n",
       "      <td>0.631653</td>\n",
       "      <td>-7.156076</td>\n",
       "      <td>0.127642</td>\n",
       "      <td>2.392122</td>\n",
       "      <td>0.097336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>phon_R01_S10_6</td>\n",
       "      <td>252.455</td>\n",
       "      <td>261.487</td>\n",
       "      <td>182.786</td>\n",
       "      <td>0.00185</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.00092</td>\n",
       "      <td>0.00113</td>\n",
       "      <td>0.00276</td>\n",
       "      <td>0.01152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01841</td>\n",
       "      <td>0.00432</td>\n",
       "      <td>26.805</td>\n",
       "      <td>0</td>\n",
       "      <td>0.610367</td>\n",
       "      <td>0.635204</td>\n",
       "      <td>-7.319510</td>\n",
       "      <td>0.200873</td>\n",
       "      <td>2.028612</td>\n",
       "      <td>0.086398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>phon_R01_S13_1</td>\n",
       "      <td>122.188</td>\n",
       "      <td>128.611</td>\n",
       "      <td>115.765</td>\n",
       "      <td>0.00524</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.00169</td>\n",
       "      <td>0.00203</td>\n",
       "      <td>0.00507</td>\n",
       "      <td>0.01613</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02566</td>\n",
       "      <td>0.00839</td>\n",
       "      <td>23.162</td>\n",
       "      <td>0</td>\n",
       "      <td>0.579597</td>\n",
       "      <td>0.733659</td>\n",
       "      <td>-6.439398</td>\n",
       "      <td>0.266392</td>\n",
       "      <td>2.079922</td>\n",
       "      <td>0.133867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              name  MDVP:Fo(Hz)  MDVP:Fhi(Hz)  MDVP:Flo(Hz)  MDVP:Jitter(%)  \\\n",
       "0   phon_R01_S01_1      119.992       157.302        74.997         0.00784   \n",
       "1   phon_R01_S01_2      122.400       148.650       113.819         0.00968   \n",
       "2   phon_R01_S01_3      116.682       131.111       111.555         0.01050   \n",
       "3   phon_R01_S01_4      116.676       137.871       111.366         0.00997   \n",
       "4   phon_R01_S01_5      116.014       141.781       110.655         0.01284   \n",
       "5   phon_R01_S01_6      120.552       131.162       113.787         0.00968   \n",
       "6   phon_R01_S02_1      120.267       137.244       114.820         0.00333   \n",
       "7   phon_R01_S02_2      107.332       113.840       104.315         0.00290   \n",
       "8   phon_R01_S02_3       95.730       132.068        91.754         0.00551   \n",
       "9   phon_R01_S02_4       95.056       120.103        91.226         0.00532   \n",
       "10  phon_R01_S02_5       88.333       112.240        84.072         0.00505   \n",
       "11  phon_R01_S02_6       91.904       115.871        86.292         0.00540   \n",
       "12  phon_R01_S04_1      136.926       159.866       131.276         0.00293   \n",
       "13  phon_R01_S04_2      139.173       179.139        76.556         0.00390   \n",
       "14  phon_R01_S04_3      152.845       163.305        75.836         0.00294   \n",
       "15  phon_R01_S04_4      142.167       217.455        83.159         0.00369   \n",
       "16  phon_R01_S04_5      144.188       349.259        82.764         0.00544   \n",
       "17  phon_R01_S04_6      168.778       232.181        75.603         0.00718   \n",
       "18  phon_R01_S05_1      153.046       175.829        68.623         0.00742   \n",
       "19  phon_R01_S05_2      156.405       189.398       142.822         0.00768   \n",
       "20  phon_R01_S05_3      153.848       165.738        65.782         0.00840   \n",
       "21  phon_R01_S05_4      153.880       172.860        78.128         0.00480   \n",
       "22  phon_R01_S05_5      167.930       193.221        79.068         0.00442   \n",
       "23  phon_R01_S05_6      173.917       192.735        86.180         0.00476   \n",
       "24  phon_R01_S06_1      163.656       200.841        76.779         0.00742   \n",
       "25  phon_R01_S06_2      104.400       206.002        77.968         0.00633   \n",
       "26  phon_R01_S06_3      171.041       208.313        75.501         0.00455   \n",
       "27  phon_R01_S06_4      146.845       208.701        81.737         0.00496   \n",
       "28  phon_R01_S06_5      155.358       227.383        80.055         0.00310   \n",
       "29  phon_R01_S06_6      162.568       198.346        77.630         0.00502   \n",
       "30  phon_R01_S07_1      197.076       206.896       192.055         0.00289   \n",
       "31  phon_R01_S07_2      199.228       209.512       192.091         0.00241   \n",
       "32  phon_R01_S07_3      198.383       215.203       193.104         0.00212   \n",
       "33  phon_R01_S07_4      202.266       211.604       197.079         0.00180   \n",
       "34  phon_R01_S07_5      203.184       211.526       196.160         0.00178   \n",
       "35  phon_R01_S07_6      201.464       210.565       195.708         0.00198   \n",
       "36  phon_R01_S08_1      177.876       192.921       168.013         0.00411   \n",
       "37  phon_R01_S08_2      176.170       185.604       163.564         0.00369   \n",
       "38  phon_R01_S08_3      180.198       201.249       175.456         0.00284   \n",
       "39  phon_R01_S08_4      187.733       202.324       173.015         0.00316   \n",
       "40  phon_R01_S08_5      186.163       197.724       177.584         0.00298   \n",
       "41  phon_R01_S08_6      184.055       196.537       166.977         0.00258   \n",
       "42  phon_R01_S10_1      237.226       247.326       225.227         0.00298   \n",
       "43  phon_R01_S10_2      241.404       248.834       232.483         0.00281   \n",
       "44  phon_R01_S10_3      243.439       250.912       232.435         0.00210   \n",
       "45  phon_R01_S10_4      242.852       255.034       227.911         0.00225   \n",
       "46  phon_R01_S10_5      245.510       262.090       231.848         0.00235   \n",
       "47  phon_R01_S10_6      252.455       261.487       182.786         0.00185   \n",
       "48  phon_R01_S13_1      122.188       128.611       115.765         0.00524   \n",
       "\n",
       "    MDVP:Jitter(Abs)  MDVP:RAP  MDVP:PPQ  Jitter:DDP  MDVP:Shimmer  ...  \\\n",
       "0           0.000070   0.00370   0.00554     0.01109       0.04374  ...   \n",
       "1           0.000080   0.00465   0.00696     0.01394       0.06134  ...   \n",
       "2           0.000090   0.00544   0.00781     0.01633       0.05233  ...   \n",
       "3           0.000090   0.00502   0.00698     0.01505       0.05492  ...   \n",
       "4           0.000110   0.00655   0.00908     0.01966       0.06425  ...   \n",
       "5           0.000080   0.00463   0.00750     0.01388       0.04701  ...   \n",
       "6           0.000030   0.00155   0.00202     0.00466       0.01608  ...   \n",
       "7           0.000030   0.00144   0.00182     0.00431       0.01567  ...   \n",
       "8           0.000060   0.00293   0.00332     0.00880       0.02093  ...   \n",
       "9           0.000060   0.00268   0.00332     0.00803       0.02838  ...   \n",
       "10          0.000060   0.00254   0.00330     0.00763       0.02143  ...   \n",
       "11          0.000060   0.00281   0.00336     0.00844       0.02752  ...   \n",
       "12          0.000020   0.00118   0.00153     0.00355       0.01259  ...   \n",
       "13          0.000030   0.00165   0.00208     0.00496       0.01642  ...   \n",
       "14          0.000020   0.00121   0.00149     0.00364       0.01828  ...   \n",
       "15          0.000030   0.00157   0.00203     0.00471       0.01503  ...   \n",
       "16          0.000040   0.00211   0.00292     0.00632       0.02047  ...   \n",
       "17          0.000040   0.00284   0.00387     0.00853       0.03327  ...   \n",
       "18          0.000050   0.00364   0.00432     0.01092       0.05517  ...   \n",
       "19          0.000050   0.00372   0.00399     0.01116       0.03995  ...   \n",
       "20          0.000050   0.00428   0.00450     0.01285       0.03810  ...   \n",
       "21          0.000030   0.00232   0.00267     0.00696       0.04137  ...   \n",
       "22          0.000030   0.00220   0.00247     0.00661       0.04351  ...   \n",
       "23          0.000030   0.00221   0.00258     0.00663       0.04192  ...   \n",
       "24          0.000050   0.00380   0.00390     0.01140       0.01659  ...   \n",
       "25          0.000060   0.00316   0.00375     0.00948       0.03767  ...   \n",
       "26          0.000030   0.00250   0.00234     0.00750       0.01966  ...   \n",
       "27          0.000030   0.00250   0.00275     0.00749       0.01919  ...   \n",
       "28          0.000020   0.00159   0.00176     0.00476       0.01718  ...   \n",
       "29          0.000030   0.00280   0.00253     0.00841       0.01791  ...   \n",
       "30          0.000010   0.00166   0.00168     0.00498       0.01098  ...   \n",
       "31          0.000010   0.00134   0.00138     0.00402       0.01015  ...   \n",
       "32          0.000010   0.00113   0.00135     0.00339       0.01263  ...   \n",
       "33          0.000009   0.00093   0.00107     0.00278       0.00954  ...   \n",
       "34          0.000009   0.00094   0.00106     0.00283       0.00958  ...   \n",
       "35          0.000010   0.00105   0.00115     0.00314       0.01194  ...   \n",
       "36          0.000020   0.00233   0.00241     0.00700       0.02126  ...   \n",
       "37          0.000020   0.00205   0.00218     0.00616       0.01851  ...   \n",
       "38          0.000020   0.00153   0.00166     0.00459       0.01444  ...   \n",
       "39          0.000020   0.00168   0.00182     0.00504       0.01663  ...   \n",
       "40          0.000020   0.00165   0.00175     0.00496       0.01495  ...   \n",
       "41          0.000010   0.00134   0.00147     0.00403       0.01463  ...   \n",
       "42          0.000010   0.00169   0.00182     0.00507       0.01752  ...   \n",
       "43          0.000010   0.00157   0.00173     0.00470       0.01760  ...   \n",
       "44          0.000009   0.00109   0.00137     0.00327       0.01419  ...   \n",
       "45          0.000009   0.00117   0.00139     0.00350       0.01494  ...   \n",
       "46          0.000010   0.00127   0.00148     0.00380       0.01608  ...   \n",
       "47          0.000007   0.00092   0.00113     0.00276       0.01152  ...   \n",
       "48          0.000040   0.00169   0.00203     0.00507       0.01613  ...   \n",
       "\n",
       "    Shimmer:DDA      NHR     HNR  status      RPDE       DFA   spread1  \\\n",
       "0       0.06545  0.02211  21.033       1  0.414783  0.815285 -4.813031   \n",
       "1       0.09403  0.01929  19.085       1  0.458359  0.819521 -4.075192   \n",
       "2       0.08270  0.01309  20.651       1  0.429895  0.825288 -4.443179   \n",
       "3       0.08771  0.01353  20.644       1  0.434969  0.819235 -4.117501   \n",
       "4       0.10470  0.01767  19.649       1  0.417356  0.823484 -3.747787   \n",
       "5       0.06985  0.01222  21.378       1  0.415564  0.825069 -4.242867   \n",
       "6       0.02337  0.00607  24.886       1  0.596040  0.764112 -5.634322   \n",
       "7       0.02487  0.00344  26.892       1  0.637420  0.763262 -6.167603   \n",
       "8       0.03218  0.01070  21.812       1  0.615551  0.773587 -5.498678   \n",
       "9       0.04324  0.01022  21.862       1  0.547037  0.798463 -5.011879   \n",
       "10      0.03237  0.01166  21.118       1  0.611137  0.776156 -5.249770   \n",
       "11      0.04272  0.01141  21.414       1  0.583390  0.792520 -4.960234   \n",
       "12      0.01968  0.00581  25.703       1  0.460600  0.646846 -6.547148   \n",
       "13      0.02184  0.01041  24.889       1  0.430166  0.665833 -5.660217   \n",
       "14      0.03191  0.00609  24.922       1  0.474791  0.654027 -6.105098   \n",
       "15      0.02316  0.00839  25.175       1  0.565924  0.658245 -5.340115   \n",
       "16      0.02908  0.01859  22.333       1  0.567380  0.644692 -5.440040   \n",
       "17      0.04322  0.02919  20.376       1  0.631099  0.605417 -2.931070   \n",
       "18      0.07413  0.03160  17.280       1  0.665318  0.719467 -3.949079   \n",
       "19      0.05164  0.03365  17.153       1  0.649554  0.686080 -4.554466   \n",
       "20      0.05000  0.03871  17.536       1  0.660125  0.704087 -4.095442   \n",
       "21      0.06062  0.01849  19.493       1  0.629017  0.698951 -5.186960   \n",
       "22      0.06685  0.01280  22.468       1  0.619060  0.679834 -4.330956   \n",
       "23      0.06562  0.01840  20.422       1  0.537264  0.686894 -5.248776   \n",
       "24      0.02214  0.01778  23.831       1  0.397937  0.732479 -5.557447   \n",
       "25      0.05197  0.02887  22.066       1  0.522746  0.737948 -5.571843   \n",
       "26      0.02666  0.01095  25.908       1  0.418622  0.720916 -6.183590   \n",
       "27      0.02650  0.01328  25.119       1  0.358773  0.726652 -6.271690   \n",
       "28      0.02307  0.00677  25.970       1  0.470478  0.676258 -7.120925   \n",
       "29      0.02380  0.01170  25.678       1  0.427785  0.723797 -6.635729   \n",
       "30      0.01689  0.00339  26.775       0  0.422229  0.741367 -7.348300   \n",
       "31      0.01513  0.00167  30.940       0  0.432439  0.742055 -7.682587   \n",
       "32      0.01919  0.00119  30.775       0  0.465946  0.738703 -7.067931   \n",
       "33      0.01407  0.00072  32.684       0  0.368535  0.742133 -7.695734   \n",
       "34      0.01403  0.00065  33.047       0  0.340068  0.741899 -7.964984   \n",
       "35      0.01758  0.00135  31.732       0  0.344252  0.742737 -7.777685   \n",
       "36      0.03463  0.00586  23.216       1  0.360148  0.778834 -6.149653   \n",
       "37      0.02814  0.00340  24.951       1  0.341435  0.783626 -6.006414   \n",
       "38      0.02177  0.00231  26.738       1  0.403884  0.766209 -6.452058   \n",
       "39      0.02488  0.00265  26.310       1  0.396793  0.758324 -6.006647   \n",
       "40      0.02321  0.00231  26.822       1  0.326480  0.765623 -6.647379   \n",
       "41      0.02226  0.00257  26.453       1  0.306443  0.759203 -7.044105   \n",
       "42      0.03104  0.00740  22.736       0  0.305062  0.654172 -7.310550   \n",
       "43      0.03017  0.00675  23.145       0  0.457702  0.634267 -6.793547   \n",
       "44      0.02330  0.00454  25.368       0  0.438296  0.635285 -7.057869   \n",
       "45      0.02542  0.00476  25.032       0  0.431285  0.638928 -6.995820   \n",
       "46      0.02719  0.00476  24.602       0  0.467489  0.631653 -7.156076   \n",
       "47      0.01841  0.00432  26.805       0  0.610367  0.635204 -7.319510   \n",
       "48      0.02566  0.00839  23.162       0  0.579597  0.733659 -6.439398   \n",
       "\n",
       "     spread2        D2       PPE  \n",
       "0   0.266482  2.301442  0.284654  \n",
       "1   0.335590  2.486855  0.368674  \n",
       "2   0.311173  2.342259  0.332634  \n",
       "3   0.334147  2.405554  0.368975  \n",
       "4   0.234513  2.332180  0.410335  \n",
       "5   0.299111  2.187560  0.357775  \n",
       "6   0.257682  1.854785  0.211756  \n",
       "7   0.183721  2.064693  0.163755  \n",
       "8   0.327769  2.322511  0.231571  \n",
       "9   0.325996  2.432792  0.271362  \n",
       "10  0.391002  2.407313  0.249740  \n",
       "11  0.363566  2.642476  0.275931  \n",
       "12  0.152813  2.041277  0.138512  \n",
       "13  0.254989  2.519422  0.199889  \n",
       "14  0.203653  2.125618  0.170100  \n",
       "15  0.210185  2.205546  0.234589  \n",
       "16  0.239764  2.264501  0.218164  \n",
       "17  0.434326  3.007463  0.430788  \n",
       "18  0.357870  3.109010  0.377429  \n",
       "19  0.340176  2.856676  0.322111  \n",
       "20  0.262564  2.739710  0.365391  \n",
       "21  0.237622  2.557536  0.259765  \n",
       "22  0.262384  2.916777  0.285695  \n",
       "23  0.210279  2.547508  0.253556  \n",
       "24  0.220890  2.692176  0.215961  \n",
       "25  0.236853  2.846369  0.219514  \n",
       "26  0.226278  2.589702  0.147403  \n",
       "27  0.196102  2.314209  0.162999  \n",
       "28  0.279789  2.241742  0.108514  \n",
       "29  0.209866  1.957961  0.135242  \n",
       "30  0.177551  1.743867  0.085569  \n",
       "31  0.173319  2.103106  0.068501  \n",
       "32  0.175181  1.512275  0.096320  \n",
       "33  0.178540  1.544609  0.056141  \n",
       "34  0.163519  1.423287  0.044539  \n",
       "35  0.170183  2.447064  0.057610  \n",
       "36  0.218037  2.477082  0.165827  \n",
       "37  0.196371  2.536527  0.173218  \n",
       "38  0.212294  2.269398  0.141929  \n",
       "39  0.266892  2.382544  0.160691  \n",
       "40  0.201095  2.374073  0.130554  \n",
       "41  0.063412  2.361532  0.115730  \n",
       "42  0.098648  2.416838  0.095032  \n",
       "43  0.158266  2.256699  0.117399  \n",
       "44  0.091608  2.330716  0.091470  \n",
       "45  0.102083  2.365800  0.102706  \n",
       "46  0.127642  2.392122  0.097336  \n",
       "47  0.200873  2.028612  0.086398  \n",
       "48  0.266392  2.079922  0.133867  \n",
       "\n",
       "[49 rows x 24 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.iloc[0:98, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.iloc[0:147, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_split = df.sample(frac=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second_split = df.drop(first_split.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_split.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = first_split.sample(frac=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49, 24)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98, 24)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(147, 24)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df.iloc[147:195, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    24\n",
       "0    24\n",
       "Name: status, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4['status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With first 25% Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    36\n",
       "0    13\n",
       "Name: status, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = df1.drop(['status', 'name'], axis = 1)\n",
    "Y1 = df1.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X1_train, X1_test, Y1_train, Y1_test = train_test_split(X1, Y1, test_size = .25, random_state = 7)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X1_train)\n",
    "X1_train = scaler.transform(X1_train)\n",
    "X1_test = scaler.transform(X1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier: \n",
      "0.9230769230769231\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "KNB1 = KNeighborsClassifier()\n",
    "KNB1.fit(X1_train, Y1_train)\n",
    "KNB1_pred = KNB1.predict(X1_test)\n",
    "# summarize the fit of the model\n",
    "print(\"KNeighborsClassifier: \")\n",
    "print(metrics.accuracy_score(Y1_test, KNB1_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier: \n",
      "0.9230769230769231\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp1 = MLPClassifier(hidden_layer_sizes=(22,18,12),max_iter=1500)\n",
    "mlp1.fit(X1_train,Y1_train)\n",
    "mlp1_pred = mlp1.predict(X1_test)\n",
    "print(\"MLPClassifier: \")\n",
    "print(metrics.accuracy_score(Y1_test, mlp1_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:18:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier: \n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "XGB1 = XGBClassifier()\n",
    "XGB1.fit(X1_train, Y1_train)\n",
    "XGB1_pred = XGB1.predict(X1_test)\n",
    "print(\"XGBClassifier: \")\n",
    "print(metrics.accuracy_score(Y1_test, XGB1_pred))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6936 - accuracy: 0.5833\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.6919 - accuracy: 0.7500\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6905 - accuracy: 0.7500\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.6891 - accuracy: 0.7500\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.6878 - accuracy: 0.7500\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.6859 - accuracy: 0.7500\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6839 - accuracy: 0.7500\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 0s 998us/step - loss: 0.6816 - accuracy: 0.7500\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.6792 - accuracy: 0.7500\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6759 - accuracy: 0.7500\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.6720 - accuracy: 0.7500\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.6672 - accuracy: 0.7500\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6616 - accuracy: 0.7500\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.6544 - accuracy: 0.7500\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.6464 - accuracy: 0.7500\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6361 - accuracy: 0.7500\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.6246 - accuracy: 0.7500\n",
      "Epoch 18/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.6108 - accuracy: 0.7500\n",
      "Epoch 19/100\n",
      "4/4 [==============================] - 0s 998us/step - loss: 0.5958 - accuracy: 0.7500\n",
      "Epoch 20/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5786 - accuracy: 0.7500\n",
      "Epoch 21/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.5583 - accuracy: 0.7500\n",
      "Epoch 22/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.5410 - accuracy: 0.7500\n",
      "Epoch 23/100\n",
      "4/4 [==============================] - 0s 998us/step - loss: 0.5213 - accuracy: 0.7778\n",
      "Epoch 24/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.5001 - accuracy: 0.8056\n",
      "Epoch 25/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.4813 - accuracy: 0.8611\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.4631 - accuracy: 0.8889\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.4453 - accuracy: 0.8889\n",
      "Epoch 28/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4294 - accuracy: 0.9167\n",
      "Epoch 29/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.4145 - accuracy: 0.9444\n",
      "Epoch 30/100\n",
      "4/4 [==============================] - 0s 998us/step - loss: 0.3993 - accuracy: 0.9444\n",
      "Epoch 31/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3863 - accuracy: 0.9444\n",
      "Epoch 32/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.3745 - accuracy: 0.9444\n",
      "Epoch 33/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3628 - accuracy: 0.9444\n",
      "Epoch 34/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.3511 - accuracy: 0.9444\n",
      "Epoch 35/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3399 - accuracy: 0.9444\n",
      "Epoch 36/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.3296 - accuracy: 0.9722\n",
      "Epoch 37/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3190 - accuracy: 0.9722\n",
      "Epoch 38/100\n",
      "4/4 [==============================] - 0s 998us/step - loss: 0.3097 - accuracy: 0.9722\n",
      "Epoch 39/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2992 - accuracy: 0.9722\n",
      "Epoch 40/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2890 - accuracy: 0.9722\n",
      "Epoch 41/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2792 - accuracy: 0.9722\n",
      "Epoch 42/100\n",
      "4/4 [==============================] - 0s 998us/step - loss: 0.2697 - accuracy: 0.9722\n",
      "Epoch 43/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2611 - accuracy: 0.9722\n",
      "Epoch 44/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509 - accuracy: 0.9722\n",
      "Epoch 45/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2424 - accuracy: 0.9722\n",
      "Epoch 46/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2341 - accuracy: 0.9722\n",
      "Epoch 47/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2257 - accuracy: 0.9722\n",
      "Epoch 48/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2172 - accuracy: 0.9722\n",
      "Epoch 49/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2092 - accuracy: 0.9722\n",
      "Epoch 50/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2013 - accuracy: 0.9722\n",
      "Epoch 51/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1945 - accuracy: 0.9722\n",
      "Epoch 52/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1874 - accuracy: 0.9722\n",
      "Epoch 53/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1808 - accuracy: 0.9722\n",
      "Epoch 54/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1749 - accuracy: 0.9722\n",
      "Epoch 55/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1690 - accuracy: 0.9722\n",
      "Epoch 56/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1629 - accuracy: 0.9722\n",
      "Epoch 57/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1580 - accuracy: 0.9722\n",
      "Epoch 58/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1526 - accuracy: 0.9722\n",
      "Epoch 59/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1480 - accuracy: 0.9722\n",
      "Epoch 60/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1437 - accuracy: 0.9722\n",
      "Epoch 61/100\n",
      "4/4 [==============================] - 0s 998us/step - loss: 0.1401 - accuracy: 0.9722\n",
      "Epoch 62/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1357 - accuracy: 0.9722\n",
      "Epoch 63/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1321 - accuracy: 0.9722\n",
      "Epoch 64/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1282 - accuracy: 0.9722\n",
      "Epoch 65/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1253 - accuracy: 0.9722\n",
      "Epoch 66/100\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.1224 - accuracy: 0.9722\n",
      "Epoch 67/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1192 - accuracy: 0.9722\n",
      "Epoch 68/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1163 - accuracy: 0.9722\n",
      "Epoch 69/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1137 - accuracy: 0.9722\n",
      "Epoch 70/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1114 - accuracy: 0.9722\n",
      "Epoch 71/100\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.1089 - accuracy: 0.9722\n",
      "Epoch 72/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1067 - accuracy: 0.9722\n",
      "Epoch 73/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1043 - accuracy: 0.9722\n",
      "Epoch 74/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1025 - accuracy: 0.9722\n",
      "Epoch 75/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1008 - accuracy: 0.9722\n",
      "Epoch 76/100\n",
      "4/4 [==============================] - 0s 998us/step - loss: 0.0986 - accuracy: 0.9722\n",
      "Epoch 77/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0967 - accuracy: 0.9722\n",
      "Epoch 78/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0951 - accuracy: 0.9722\n",
      "Epoch 79/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0934 - accuracy: 0.9722\n",
      "Epoch 80/100\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.0919 - accuracy: 0.9722\n",
      "Epoch 81/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0905 - accuracy: 0.9722\n",
      "Epoch 82/100\n",
      "4/4 [==============================] - 0s 998us/step - loss: 0.0891 - accuracy: 0.9722\n",
      "Epoch 83/100\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.0595 - accuracy: 1.00 - 0s 1ms/step - loss: 0.0874 - accuracy: 0.9722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0858 - accuracy: 0.9722\n",
      "Epoch 85/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0845 - accuracy: 0.9722\n",
      "Epoch 86/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0833 - accuracy: 0.9722\n",
      "Epoch 87/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0819 - accuracy: 0.9722\n",
      "Epoch 88/100\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.0805 - accuracy: 0.9722\n",
      "Epoch 89/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0794 - accuracy: 0.9722\n",
      "Epoch 90/100\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.0781 - accuracy: 0.9722\n",
      "Epoch 91/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0769 - accuracy: 0.9722\n",
      "Epoch 92/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0758 - accuracy: 0.9722\n",
      "Epoch 93/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0748 - accuracy: 0.9722\n",
      "Epoch 94/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0735 - accuracy: 0.9722\n",
      "Epoch 95/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0725 - accuracy: 0.9722\n",
      "Epoch 96/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0714 - accuracy: 0.9722\n",
      "Epoch 97/100\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.0323 - accuracy: 1.00 - 0s 2ms/step - loss: 0.0704 - accuracy: 0.9722\n",
      "Epoch 98/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0693 - accuracy: 0.9722\n",
      "Epoch 99/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0679 - accuracy: 0.9722\n",
      "Epoch 100/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0665 - accuracy: 0.9722\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17d15bff408>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier1 = Sequential()\n",
    "#First Hidden Layer\n",
    "classifier1.add(Dense(16, activation='relu', kernel_initializer='random_normal', input_dim=22))\n",
    "#Second  Hidden Layer\n",
    "classifier1.add(Dense(8, activation='relu', kernel_initializer='random_normal'))\n",
    "#Output Layer\n",
    "classifier1.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))\n",
    "#Compiling the neural network\n",
    "classifier1.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])\n",
    "#Fitting the data to the training dataset\n",
    "classifier1.fit(X1_train,Y1_train, batch_size=10, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 0s/step - loss: 0.2136 - accuracy: 0.9231\n",
      "Test Accuracy: 92.31%\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0654 - accuracy: 0.9722\n",
      "Train Accuracy: 97.22%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "tscores1 = classifier1.evaluate(X1_test, Y1_test)\n",
    "print(\"Test Accuracy: %.2f%%\" %(tscores1[1]*100))\n",
    "\n",
    "trscores1 = classifier1.evaluate(X1_train, Y1_train)\n",
    "print(\"Train Accuracy: %.2f%%\" %(trscores1[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier : 92.3076923076923\n",
      "MLPClassifier : 92.3076923076923\n",
      "XGBClassifier : 100.0\n",
      "Sequential:  97.22222089767456\n"
     ]
    }
   ],
   "source": [
    "print('KNeighborsClassifier :', metrics.accuracy_score(Y1_test, KNB1_pred) * 100)\n",
    "print('MLPClassifier :', metrics.accuracy_score(Y1_test, mlp1_pred) * 100)\n",
    "print('XGBClassifier :', metrics.accuracy_score(Y1_test, XGB1_pred) * 100)\n",
    "print(\"Sequential: \", (trscores1[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With half of the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    74\n",
       "0    24\n",
       "Name: status, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = df2.drop(['status', 'name'], axis = 1)\n",
    "Y2 = df2.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X2_train, X2_test, Y2_train, Y2_test = train_test_split(X2, Y2, test_size = .25, random_state = 7)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X2_train)\n",
    "X2_train = scaler.transform(X2_train)\n",
    "X2_test = scaler.transform(X2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier: \n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "KNB2 = KNeighborsClassifier()\n",
    "KNB2.fit(X2_train, Y2_train)\n",
    "KNB2_pred = KNB2.predict(X2_test)\n",
    "# summarize the fit of the model\n",
    "print(\"KNeighborsClassifier: \")\n",
    "print(metrics.accuracy_score(Y2_test, KNB2_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier: \n",
      "0.96\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp2 = MLPClassifier(hidden_layer_sizes=(22,18,12),max_iter=1500)\n",
    "mlp2.fit(X2_train,Y2_train)\n",
    "mlp2_pred = mlp2.predict(X2_test)\n",
    "print(\"MLPClassifier: \")\n",
    "print(metrics.accuracy_score(Y2_test, mlp2_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:18:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier: \n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "XGB2 = XGBClassifier()\n",
    "XGB2.fit(X2_train, Y2_train)\n",
    "XGB2_pred = XGB2.predict(X2_test)\n",
    "print(\"XGBClassifier: \")\n",
    "print(metrics.accuracy_score(Y2_test, XGB2_pred))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6928 - accuracy: 0.6849\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6890 - accuracy: 0.7123\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6845 - accuracy: 0.7123\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.6784 - accuracy: 0.7123\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6686 - accuracy: 0.7123\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 873us/step - loss: 0.6544 - accuracy: 0.7123\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6341 - accuracy: 0.7123\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6086 - accuracy: 0.7123\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5723 - accuracy: 0.7123\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5344 - accuracy: 0.7123\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4974 - accuracy: 0.7123\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4640 - accuracy: 0.7123\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4335 - accuracy: 0.7260\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.4118 - accuracy: 0.8630\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3940 - accuracy: 0.8904\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.3792 - accuracy: 0.9041\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3663 - accuracy: 0.9178\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3559 - accuracy: 0.9041\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3455 - accuracy: 0.9041\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3363 - accuracy: 0.9041\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3262 - accuracy: 0.9041\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3176 - accuracy: 0.8767\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3076 - accuracy: 0.8904\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2981 - accuracy: 0.8904\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2897 - accuracy: 0.8767\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.2835 - accuracy: 0.8904\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2742 - accuracy: 0.8767\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2675 - accuracy: 0.8767\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2615 - accuracy: 0.8630\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2544 - accuracy: 0.8493\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2484 - accuracy: 0.8493\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2426 - accuracy: 0.8493\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2383 - accuracy: 0.8630\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.2330 - accuracy: 0.8767\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.2291 - accuracy: 0.8767\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2259 - accuracy: 0.8767\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2217 - accuracy: 0.8767\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2186 - accuracy: 0.8767\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2165 - accuracy: 0.8630\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.2124 - accuracy: 0.8630\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2101 - accuracy: 0.8767\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2070 - accuracy: 0.8767\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2041 - accuracy: 0.8767\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2015 - accuracy: 0.8904\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1993 - accuracy: 0.8904\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1957 - accuracy: 0.8904\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1937 - accuracy: 0.8904\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1920 - accuracy: 0.8904\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1894 - accuracy: 0.9041\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1867 - accuracy: 0.9041\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.1842 - accuracy: 0.9041\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1807 - accuracy: 0.9041\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.1784 - accuracy: 0.9041\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1754 - accuracy: 0.9178\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1724 - accuracy: 0.9178\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1698 - accuracy: 0.9178\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1671 - accuracy: 0.9178\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1642 - accuracy: 0.9178\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 873us/step - loss: 0.1625 - accuracy: 0.9178\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1585 - accuracy: 0.9178\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1555 - accuracy: 0.9315\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.1521 - accuracy: 0.9452\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.1490 - accuracy: 0.9452\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.1466 - accuracy: 0.9589\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 998us/step - loss: 0.1438 - accuracy: 0.9589\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 873us/step - loss: 0.1408 - accuracy: 0.9589\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1387 - accuracy: 0.9589\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1364 - accuracy: 0.9589\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 873us/step - loss: 0.1340 - accuracy: 0.9589\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.1306 - accuracy: 0.9589\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.1288 - accuracy: 0.9452\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1249 - accuracy: 0.9589\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.1213 - accuracy: 0.9726\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1194 - accuracy: 0.9726\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1162 - accuracy: 0.9726\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1138 - accuracy: 0.9726\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.1112 - accuracy: 0.9726\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1096 - accuracy: 0.9726\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.1066 - accuracy: 0.9863\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1043 - accuracy: 0.9863\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1016 - accuracy: 0.9863\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0980 - accuracy: 0.9863\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0965 - accuracy: 0.9863\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0935 - accuracy: 0.9863\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0904 - accuracy: 0.9863\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 998us/step - loss: 0.0888 - accuracy: 0.9863\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0870 - accuracy: 0.9863\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0841 - accuracy: 0.9863\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0825 - accuracy: 0.9726\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0819 - accuracy: 0.9726\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0791 - accuracy: 0.9726\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0768 - accuracy: 0.9726\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0750 - accuracy: 0.9726\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0734 - accuracy: 0.9863\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0722 - accuracy: 0.9863\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0705 - accuracy: 0.9726\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0685 - accuracy: 0.9726\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0672 - accuracy: 0.9863\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0659 - accuracy: 0.9863\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 873us/step - loss: 0.0649 - accuracy: 0.9863\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17d16e8a188>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier2 = Sequential()\n",
    "#First Hidden Layer\n",
    "classifier2.add(Dense(16, activation='relu', kernel_initializer='random_normal', input_dim=22))\n",
    "#Second  Hidden Layer\n",
    "classifier2.add(Dense(8, activation='relu', kernel_initializer='random_normal'))\n",
    "#Output Layer\n",
    "classifier2.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))\n",
    "#Compiling the neural network\n",
    "classifier2.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])\n",
    "#Fitting the data to the training dataset\n",
    "classifier2.fit(X2_train,Y2_train, batch_size=10, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 0s/step - loss: 0.0968 - accuracy: 0.9600\n",
      "Test Accuracy: 96.00%\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0631 - accuracy: 0.9863\n",
      "Train Accuracy: 98.63%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "tscores2 = classifier2.evaluate(X2_test, Y2_test)\n",
    "print(\"Test Accuracy: %.2f%%\" %(tscores2[1]*100))\n",
    "\n",
    "trscores2 = classifier2.evaluate(X2_train, Y2_train)\n",
    "print(\"Train Accuracy: %.2f%%\" %(trscores2[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier : 100.0\n",
      "MLPClassifier : 96.0\n",
      "XGBClassifier : 100.0\n",
      "Sequential:  98.63013625144958\n"
     ]
    }
   ],
   "source": [
    "print('KNeighborsClassifier :', metrics.accuracy_score(Y2_test, KNB2_pred) * 100)\n",
    "print('MLPClassifier :', metrics.accuracy_score(Y2_test, mlp2_pred) * 100)\n",
    "print('XGBClassifier :', metrics.accuracy_score(Y2_test, XGB2_pred) * 100)\n",
    "print(\"Sequential: \", (trscores2[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With 75% Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    123\n",
       "0     24\n",
       "Name: status, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3['status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    123\n",
       "0     24\n",
       "Name: status, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3 = df3.drop(['status', 'name'], axis = 1)\n",
    "Y3 = df3.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X3_train, X3_test, Y3_train, Y3_test = train_test_split(X3, Y3, test_size = .25, random_state = 7)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X3_train)\n",
    "X3_train = scaler.transform(X3_train)\n",
    "X3_test = scaler.transform(X3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier: \n",
      "0.972972972972973\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "KNB3 = KNeighborsClassifier()\n",
    "KNB3.fit(X3_train, Y3_train)\n",
    "KNB3_pred = KNB3.predict(X3_test)\n",
    "# summarize the fit of the model\n",
    "print(\"KNeighborsClassifier: \")\n",
    "print(metrics.accuracy_score(Y3_test, KNB3_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier: \n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp3 = MLPClassifier(hidden_layer_sizes=(22,18,12),max_iter=1500)\n",
    "mlp3.fit(X3_train,Y3_train)\n",
    "mlp3_pred = mlp3.predict(X3_test)\n",
    "print(\"MLPClassifier: \")\n",
    "print(metrics.accuracy_score(Y3_test, mlp3_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:18:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier: \n",
      "0.8918918918918919\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "XGB3 = XGBClassifier()\n",
    "XGB3.fit(X3_train, Y3_train)\n",
    "XGB3_pred = XGB3.predict(X3_test)\n",
    "print(\"XGBClassifier: \")\n",
    "print(metrics.accuracy_score(Y3_test, XGB3_pred))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 0.6910 - accuracy: 0.8727\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.6864 - accuracy: 0.9364\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.6804 - accuracy: 0.9273\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.6731 - accuracy: 0.9273\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.6632 - accuracy: 0.9091\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.6479 - accuracy: 0.9455\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.6264 - accuracy: 0.9455\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5987 - accuracy: 0.9455\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5622 - accuracy: 0.9455\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5240 - accuracy: 0.9636\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.4841 - accuracy: 0.9636\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.4431 - accuracy: 0.9455\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.4068 - accuracy: 0.9545\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3727 - accuracy: 0.9545\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3435 - accuracy: 0.9545\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.3158 - accuracy: 0.9455\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.2908 - accuracy: 0.9455\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.2679 - accuracy: 0.9455\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.2455 - accuracy: 0.9545\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.2277 - accuracy: 0.9455\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.2058 - accuracy: 0.9545\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1895 - accuracy: 0.9545\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1743 - accuracy: 0.9545\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1615 - accuracy: 0.9727\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1504 - accuracy: 0.9545\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1393 - accuracy: 0.9727\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1310 - accuracy: 0.9727\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1232 - accuracy: 0.9727\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1171 - accuracy: 0.9727\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1109 - accuracy: 0.9818\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1055 - accuracy: 0.9818\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.1004 - accuracy: 0.9727\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0969 - accuracy: 0.9727\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0921 - accuracy: 0.9727\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0892 - accuracy: 0.9818\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 0s 999us/step - loss: 0.0857 - accuracy: 0.9818\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 0s 815us/step - loss: 0.0839 - accuracy: 0.9818\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0801 - accuracy: 0.9818\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0780 - accuracy: 0.9818\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0757 - accuracy: 0.9818\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0745 - accuracy: 0.9818\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0718 - accuracy: 0.9909\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0699 - accuracy: 0.9909\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0679 - accuracy: 0.9909\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0662 - accuracy: 0.9909\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0647 - accuracy: 0.9909\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 0s 998us/step - loss: 0.0635 - accuracy: 0.9909\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0624 - accuracy: 0.9909\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0609 - accuracy: 0.9909\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0596 - accuracy: 0.9909\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 0s 998us/step - loss: 0.0579 - accuracy: 0.9909\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0566 - accuracy: 0.9909\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0562 - accuracy: 0.9909\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0553 - accuracy: 0.9909\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0871 - accuracy: 1.00 - 0s 997us/step - loss: 0.0539 - accuracy: 0.9909\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 0s 906us/step - loss: 0.0525 - accuracy: 0.9909\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0519 - accuracy: 0.9909\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 0s 907us/step - loss: 0.0508 - accuracy: 0.9909\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0497 - accuracy: 0.9909\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0492 - accuracy: 0.9909\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 0s 816us/step - loss: 0.0482 - accuracy: 0.9909\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0477 - accuracy: 0.9909\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0469 - accuracy: 0.9909\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 0s 907us/step - loss: 0.0459 - accuracy: 0.9909\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 0s 907us/step - loss: 0.0447 - accuracy: 0.9909\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 0s 816us/step - loss: 0.0440 - accuracy: 0.9909\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0435 - accuracy: 0.9909\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 0s 998us/step - loss: 0.0428 - accuracy: 0.9909\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 0s 907us/step - loss: 0.0423 - accuracy: 0.9909\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0413 - accuracy: 0.9909\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 0s 907us/step - loss: 0.0409 - accuracy: 0.9909\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0402 - accuracy: 0.9909\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 0s 907us/step - loss: 0.0397 - accuracy: 0.9909\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0389 - accuracy: 0.9909\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 0s 907us/step - loss: 0.0387 - accuracy: 0.9909\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 0s 907us/step - loss: 0.0379 - accuracy: 0.9909\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 0s 907us/step - loss: 0.0368 - accuracy: 0.9909\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 0s 907us/step - loss: 0.0361 - accuracy: 0.9909\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0333 - accuracy: 0.9909\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 0s 907us/step - loss: 0.0322 - accuracy: 0.9909\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 0s 907us/step - loss: 0.0308 - accuracy: 0.9909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100\n",
      "11/11 [==============================] - 0s 906us/step - loss: 0.0299 - accuracy: 0.9909\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 0s 906us/step - loss: 0.0304 - accuracy: 0.9909\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0277 - accuracy: 0.9909\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0269 - accuracy: 0.9909\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 0s 816us/step - loss: 0.0261 - accuracy: 0.9909\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 0s 907us/step - loss: 0.0250 - accuracy: 0.9909\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 0s 907us/step - loss: 0.0242 - accuracy: 0.9909\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 0s 998us/step - loss: 0.0238 - accuracy: 0.9909\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 0s 907us/step - loss: 0.0232 - accuracy: 0.9909\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 0s 907us/step - loss: 0.0219 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 0s 816us/step - loss: 0.0212 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 0s 816us/step - loss: 0.0207 - accuracy: 0.9909\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 0s 907us/step - loss: 0.0202 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 0s 907us/step - loss: 0.0193 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0188 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0183 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0175 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 0s 907us/step - loss: 0.0173 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 0s 907us/step - loss: 0.0173 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17d16f900c8>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier3 = Sequential()\n",
    "#First Hidden Layer\n",
    "classifier3.add(Dense(16, activation='relu', kernel_initializer='random_normal', input_dim=22))\n",
    "#Second  Hidden Layer\n",
    "classifier3.add(Dense(8, activation='relu', kernel_initializer='random_normal'))\n",
    "#Output Layer\n",
    "classifier3.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))\n",
    "#Compiling the neural network\n",
    "classifier3.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])\n",
    "#Fitting the data to the training dataset\n",
    "classifier3.fit(X3_train,Y3_train, batch_size=10, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 0s/step - loss: 0.0794 - accuracy: 0.9730\n",
      "Test Accuracy: 97.30%\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.0161 - accuracy: 1.0000\n",
      "Train Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "tscores3 = classifier3.evaluate(X3_test, Y3_test)\n",
    "print(\"Test Accuracy: %.2f%%\" %(tscores3[1]*100))\n",
    "\n",
    "trscores3 = classifier3.evaluate(X3_train, Y3_train)\n",
    "print(\"Train Accuracy: %.2f%%\" %(trscores3[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier : 97.2972972972973\n",
      "MLPClassifier : 100.0\n",
      "XGBClassifier : 89.1891891891892\n",
      "Sequential:  100.0\n"
     ]
    }
   ],
   "source": [
    "print('KNeighborsClassifier :', metrics.accuracy_score(Y3_test, KNB3_pred) * 100)\n",
    "print('MLPClassifier :', metrics.accuracy_score(Y3_test, mlp3_pred) * 100)\n",
    "print('XGBClassifier :', metrics.accuracy_score(Y3_test, XGB3_pred) * 100)\n",
    "print(\"Sequential: \", (trscores3[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With 100% Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['status', 'name'], axis = 1)\n",
    "Y = df.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = .25, random_state = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler  \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier: \n",
      "0.9795918367346939\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "KNB = KNeighborsClassifier(n_neighbors = 5)\n",
    "KNB.fit(X_train, Y_train)\n",
    "KNB_pred = KNB.predict(X_test)\n",
    "# summarize the fit of the model\n",
    "print(\"KNeighborsClassifier: \")\n",
    "print(metrics.accuracy_score(Y_test, KNB_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier: \n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(22,18,12),max_iter=1500)\n",
    "mlp.fit(X_train,Y_train)\n",
    "mlp_pred = mlp.predict(X_test)\n",
    "print(\"MLPClassifier: \")\n",
    "print(metrics.accuracy_score(Y_test, mlp_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:18:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier: \n",
      "0.9795918367346939\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "XGB = XGBClassifier()\n",
    "XGB.fit(X_train, Y_train)\n",
    "XGB_pred = XGB.predict(X_test)\n",
    "print(\"XGBClassifier: \")\n",
    "print(metrics.accuracy_score(Y_test, XGB_pred))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.6911 - accuracy: 0.7397\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.6866 - accuracy: 0.7397\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.6806 - accuracy: 0.7397\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.6723 - accuracy: 0.7603\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.6584 - accuracy: 0.8288\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.6388 - accuracy: 0.8493\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 0s 998us/step - loss: 0.6057 - accuracy: 0.8562\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.5667 - accuracy: 0.8630\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.5197 - accuracy: 0.8630\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.4757 - accuracy: 0.8630\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.4353 - accuracy: 0.8630\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.4046 - accuracy: 0.8562\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.3763 - accuracy: 0.8699\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.3550 - accuracy: 0.8699\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.3374 - accuracy: 0.8767\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.3232 - accuracy: 0.8767\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.3108 - accuracy: 0.8767\n",
      "Epoch 18/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.3011 - accuracy: 0.8767\n",
      "Epoch 19/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.2910 - accuracy: 0.8767\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.2835 - accuracy: 0.8767\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.2760 - accuracy: 0.8767\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.2706 - accuracy: 0.8767\n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.2646 - accuracy: 0.8767\n",
      "Epoch 24/100\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.2581 - accuracy: 0.8767\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.2534 - accuracy: 0.8836\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.2485 - accuracy: 0.8836\n",
      "Epoch 27/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.2439 - accuracy: 0.8836\n",
      "Epoch 28/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2394 - accuracy: 0.8836\n",
      "Epoch 29/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.2351 - accuracy: 0.8836\n",
      "Epoch 30/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.2307 - accuracy: 0.8836\n",
      "Epoch 31/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.2270 - accuracy: 0.8836\n",
      "Epoch 32/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.2225 - accuracy: 0.8767\n",
      "Epoch 33/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.2190 - accuracy: 0.8836\n",
      "Epoch 34/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.2144 - accuracy: 0.8973\n",
      "Epoch 35/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.2133 - accuracy: 0.8904\n",
      "Epoch 36/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.2084 - accuracy: 0.8973\n",
      "Epoch 37/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.2043 - accuracy: 0.8973\n",
      "Epoch 38/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.2015 - accuracy: 0.8973\n",
      "Epoch 39/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1980 - accuracy: 0.8973\n",
      "Epoch 40/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1950 - accuracy: 0.9041\n",
      "Epoch 41/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1920 - accuracy: 0.9041\n",
      "Epoch 42/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1908 - accuracy: 0.9041\n",
      "Epoch 43/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1855 - accuracy: 0.9110\n",
      "Epoch 44/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1823 - accuracy: 0.9110\n",
      "Epoch 45/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1797 - accuracy: 0.9041\n",
      "Epoch 46/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1773 - accuracy: 0.9041\n",
      "Epoch 47/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1732 - accuracy: 0.9041\n",
      "Epoch 48/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1713 - accuracy: 0.9178\n",
      "Epoch 49/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1682 - accuracy: 0.9110\n",
      "Epoch 50/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1652 - accuracy: 0.9178\n",
      "Epoch 51/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1625 - accuracy: 0.9178\n",
      "Epoch 52/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1594 - accuracy: 0.9247\n",
      "Epoch 53/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1586 - accuracy: 0.9178\n",
      "Epoch 54/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1547 - accuracy: 0.9178\n",
      "Epoch 55/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1522 - accuracy: 0.9178\n",
      "Epoch 56/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1495 - accuracy: 0.9178\n",
      "Epoch 57/100\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.1148 - accuracy: 1.00 - 0s 1ms/step - loss: 0.1470 - accuracy: 0.9247\n",
      "Epoch 58/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1453 - accuracy: 0.9247\n",
      "Epoch 59/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1423 - accuracy: 0.9247\n",
      "Epoch 60/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1406 - accuracy: 0.9247\n",
      "Epoch 61/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1380 - accuracy: 0.9247\n",
      "Epoch 62/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1357 - accuracy: 0.9247\n",
      "Epoch 63/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1331 - accuracy: 0.9247\n",
      "Epoch 64/100\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.1313 - accuracy: 0.9247\n",
      "Epoch 65/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1299 - accuracy: 0.9247\n",
      "Epoch 66/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1268 - accuracy: 0.9247\n",
      "Epoch 67/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1249 - accuracy: 0.9247\n",
      "Epoch 68/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1221 - accuracy: 0.9315\n",
      "Epoch 69/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1209 - accuracy: 0.9315\n",
      "Epoch 70/100\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.1205 - accuracy: 0.9384\n",
      "Epoch 71/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1156 - accuracy: 0.9452\n",
      "Epoch 72/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1132 - accuracy: 0.9521\n",
      "Epoch 73/100\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0780 - accuracy: 1.00 - 0s 1ms/step - loss: 0.1111 - accuracy: 0.9521\n",
      "Epoch 74/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1085 - accuracy: 0.9589\n",
      "Epoch 75/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1072 - accuracy: 0.9589\n",
      "Epoch 76/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1040 - accuracy: 0.9658\n",
      "Epoch 77/100\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.0309 - accuracy: 1.00 - 0s 1ms/step - loss: 0.1019 - accuracy: 0.9658\n",
      "Epoch 78/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1004 - accuracy: 0.9726\n",
      "Epoch 79/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0975 - accuracy: 0.9795\n",
      "Epoch 80/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0954 - accuracy: 0.9863\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0935 - accuracy: 0.9863\n",
      "Epoch 82/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0918 - accuracy: 0.9932\n",
      "Epoch 83/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0885 - accuracy: 0.9932\n",
      "Epoch 84/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0871 - accuracy: 0.9932\n",
      "Epoch 85/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0857 - accuracy: 0.9932\n",
      "Epoch 86/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0830 - accuracy: 0.9932\n",
      "Epoch 87/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0814 - accuracy: 0.9932\n",
      "Epoch 88/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0795 - accuracy: 0.9932\n",
      "Epoch 89/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0771 - accuracy: 0.9932\n",
      "Epoch 90/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0759 - accuracy: 0.9932\n",
      "Epoch 91/100\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.0744 - accuracy: 0.9932\n",
      "Epoch 92/100\n",
      "15/15 [==============================] - 0s 931us/step - loss: 0.0727 - accuracy: 0.9932\n",
      "Epoch 93/100\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.0703 - accuracy: 0.9932\n",
      "Epoch 94/100\n",
      "15/15 [==============================] - 0s 931us/step - loss: 0.0689 - accuracy: 0.9932\n",
      "Epoch 95/100\n",
      "15/15 [==============================] - 0s 931us/step - loss: 0.0671 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "15/15 [==============================] - 0s 931us/step - loss: 0.0656 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "15/15 [==============================] - 0s 931us/step - loss: 0.0641 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0652 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.0597 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "15/15 [==============================] - 0s 931us/step - loss: 0.0609 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17d1809be48>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = Sequential()\n",
    "#First Hidden Layer\n",
    "classifier.add(Dense(16, activation='relu', kernel_initializer='random_normal', input_dim=22))\n",
    "#Second  Hidden Layer\n",
    "classifier.add(Dense(8, activation='relu', kernel_initializer='random_normal'))\n",
    "#Output Layer\n",
    "classifier.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))\n",
    "#Compiling the neural network\n",
    "classifier.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])\n",
    "#Fitting the data to the training dataset\n",
    "classifier.fit(X_train,Y_train, batch_size=10, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 0s/step - loss: 0.0371 - accuracy: 1.0000\n",
      "Test Accuracy: 100.00%\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0488 - accuracy: 1.0000\n",
      "Train Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "tscores = classifier.evaluate(X3_test, Y3_test)\n",
    "print(\"Test Accuracy: %.2f%%\" %(tscores[1]*100))\n",
    "\n",
    "trscores=classifier.evaluate(X3_train, Y3_train)\n",
    "print(\"Train Accuracy: %.2f%%\" %(trscores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier : 97.95918367346938\n",
      "MLPClassifier : 100.0\n",
      "XGBClassifier : 97.95918367346938\n",
      "Sequential:  100.0\n"
     ]
    }
   ],
   "source": [
    "print('KNeighborsClassifier :', metrics.accuracy_score(Y_test, KNB_pred) * 100)\n",
    "print('MLPClassifier :', metrics.accuracy_score(Y_test, mlp_pred) * 100)\n",
    "print('XGBClassifier :', metrics.accuracy_score(Y_test, XGB_pred) * 100)\n",
    "print(\"Sequential: \", (trscores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
